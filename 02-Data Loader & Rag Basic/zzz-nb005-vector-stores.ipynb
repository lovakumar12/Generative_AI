{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "465c003a-29cf-4cfe-a0c6-d36c04ae2b37",
   "metadata": {},
   "source": [
    "# Vector Stores (aka. Vector Databases)\n",
    "* Store embeddings in a very fast searchable database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5514113-ddca-4ae9-9de6-0b9225b18f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fecd39d0-e72e-4bc2-8a68-2fa4008ea365",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f4a923-b19e-498e-9be5-e47ec4a77d80",
   "metadata": {},
   "source": [
    "#### Install LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fef1e5c-b7e2-4a04-96c5-8f64377b8eba",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1cf94ae-6c39-4475-9c5b-4b74d8d78753",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189e9e17-dfb0-4fd3-85b9-1fba83771941",
   "metadata": {},
   "source": [
    "## Connect with an LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21d23f4-61f5-4227-8a75-7eefde6680ee",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "148df8e0-361d-4ddd-8709-af48fa1648d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1998155-91de-4cbc-bc88-8d77beefb51b",
   "metadata": {},
   "source": [
    "* NOTE: Since right now is the best LLM in the market, we will use OpenAI by default. You will see how to connect with other Open Source LLMs like Llama3 or Mistral in a next lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "209b683b-5edc-4418-9468-34c12f1172d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chatModel = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c811b84c-7054-49dd-b0ac-30f327f648a0",
   "metadata": {},
   "source": [
    "## Reminder: Steps of the RAG process.\n",
    "* When you load a document, you end up with strings. Sometimes the strings will be too large to fit into the context window. In those occassions we will use the RAG technique:\n",
    "    * Split document in small chunks.\n",
    "    * Transform text chunks in numeric chunks (embeddings).\n",
    "    * **Load embeddings to a vector database (aka vector store)**.\n",
    "    * Load question and retrieve the most relevant embeddings to respond it.\n",
    "    * Sent the embeddings to the LLM to format the response properly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c573cdd8-3aff-488b-8649-4050a42e26c4",
   "metadata": {},
   "source": [
    "## Vector databases (aka vector stores): store and search embeddings\n",
    "* See the documentation page [here](https://python.langchain.com/v0.1/docs/modules/data_connection/vectorstores/).\n",
    "* See the list of vector stores [here](https://python.langchain.com/v0.1/docs/integrations/vectorstores/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055b5f44-5c40-4eb7-aa25-f1591b0f1580",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25bbb79b-595f-44e8-b146-65152becb321",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain-chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e0c5a1d-10d3-4813-a5e3-765c2127e174",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "# Load the document, split it into chunks, embed each chunk and load it into the vector store.\n",
    "loaded_document = TextLoader('./data/state_of_the_union.txt').load()\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "\n",
    "chunks_of_text = text_splitter.split_documents(loaded_document)\n",
    "\n",
    "vector_db = Chroma.from_documents(chunks_of_text, OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdbd14f2-a3ca-40db-887b-b86d65d0b22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you’re at it, pass the Disclose Act so Americans can know who is funding our elections. \n",
      "\n",
      "Tonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \n",
      "\n",
      "One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \n",
      "\n",
      "And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.\n"
     ]
    }
   ],
   "source": [
    "question = \"What did the president say about the John Lewis Voting Rights Act?\"\n",
    "\n",
    "response = vector_db.similarity_search(question)\n",
    "\n",
    "print(response[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d43131-adf6-4d17-a256-05432a834b29",
   "metadata": {},
   "source": [
    "The `.similarity_search` method in the previous code is used to find the most relevant text chunks from a pre-processed document that are similar to a given query. Here's how it works step-by-step\n",
    "\n",
    "1. **Document Processing and Embedding:**\n",
    "   - The document `state_of_the_union.txt` is loaded using the `TextLoader` from the LangChain Community package.\n",
    "   - The document is then split into smaller chunks of text using the `CharacterTextSplitter`, where each chunk is 1000 characters long without any overlap between the chunks.\n",
    "   - Each chunk of text is then transformed into an embedding using `OpenAIEmbeddings`. Embeddings are high-dimensional vectors that represent the semantic content of the text.\n",
    "   - These embeddings are stored in an instance of `Chroma`, which serves as a vector database optimized for efficient similarity searches.\n",
    "\n",
    "2. **Using `.similarity_search`:**\n",
    "   - When you invoke `vector_db.similarity_search(question)`, the method converts the query (`\"What did the president say about the John Lewis Voting Rights Act?\"`) into an embedding using the same method that was used for the chunks.\n",
    "   - It then searches the vector database (`vector_db`) to find the chunks whose embeddings are most similar to the embedding of the query. This similarity is typically measured using metrics such as cosine similarity.\n",
    "   - The search results are sorted by relevance, with the most relevant chunks (those that are semantically closest to the query) returned first.\n",
    "\n",
    "3. **Output:**\n",
    "   - The result of the `.similarity_search` is stored in `response`, which contains the relevant chunks and their similarity scores.\n",
    "   - The script prints the content of the most relevant chunk (the first result), which should ideally contain information about what the president said regarding the John Lewis Voting Rights Act.\n",
    "\n",
    "This method is particularly useful in applications like question answering or document retrieval where you need to quickly find the most relevant parts of a large text based on a query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7df8f9",
   "metadata": {},
   "source": [
    "## chroma db with hugging face embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f8a7df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1008c018",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the document, split it into chunks, embed each chunk and load it into the vector store.\n",
    "loaded_document = PyPDFLoader('data1\\pages.pdf').load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "\n",
    "chunks_of_text = text_splitter.split_documents(loaded_document)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db8ffc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_model_name = 'BAAI/bge-large-en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aca6f7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=embeddings_model_name)\n",
    "#embed=embeddings.embed_documents(chunks_of_text)\n",
    "vector_db = Chroma.from_documents(chunks_of_text, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6a36eef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 3 of 4 Creating PDF Documents (continued) \n",
      " \n",
      "Option 2: If you do not have  Acrobat Standard or higher \n",
      "installed use PS2PSF.*   \n",
      "    \n",
      " 1. Open the file in its authoring app lication, and choose File > Print. \n",
      "2. Select “Print to File” and save. \n",
      "3. Open your browser and go to http://ps2pdf.com/convert.htm\n",
      " \n",
      "4. Click “browse” select the file you created in step 2 (.prn or .ps), \n",
      "click “convert” \n",
      "5. Download the newly created PDF file. \n",
      "*Note: Some formatting changes ma y occur once converted (bullets \n",
      "may turn to symbols and color may become black and white).\n"
     ]
    }
   ],
   "source": [
    "question = \" give me the stpes to crete a pdf document\"\n",
    "\n",
    "response = vector_db.similarity_search(question)\n",
    "\n",
    "print(response[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bbd8a6da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 4 of 4 Reducing File Size Options \n",
      " \n",
      "*WebDCU will accept files up to 2.0MB.* \n",
      " Here is a rough estimate for PDF file sizes: If the contents are pure text, like a CV, the file size is usually 10kb per \n",
      "page; therefore, a 1MB file will ha ve about 100 pages.  If the file \n",
      "includes some pictures, the file size may increase. If the file is a \n",
      "picture, like a scanned license or ce rtification, you may have different \n",
      "file sizes based on the picture quality.  In most cases, saving the file at \n",
      "about 250kb per page should be enough to generate a clear picture.  Option 1 – Use Adobe PDF Print Command: 1. Open the PDF file, and choose File > Print. 2. Choose Adobe PDF from the printer menu next to Name. 3. Click the Properties (or Preferen ces) button to customize the Adobe \n",
      "PDF printer setting. (In some app lications, you may need to click \n",
      "Setup in the Print dialog box to open  the list of printers, and then click \n",
      "Properties or Preferences.)  Choose Sm allest File Size as your default\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "question = \" how to reduce the size of a pdf document\"\n",
    "\n",
    "response = vector_db.similarity_search(question)\n",
    "\n",
    "print(response[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47224219",
   "metadata": {},
   "source": [
    "## milvus with hugging face embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c052f1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_milvus import Milvus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "430ccc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=PyPDFLoader('data1\\pages.pdf').load()\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "chunks_of_text=text_splitter.split_documents(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b88b0774",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kumar\\.conda\\envs\\myenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "model=\"BAAI/bge-large-en\"\n",
    "embeddings=HuggingFaceEmbeddings(model_name=model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16131b6",
   "metadata": {},
   "source": [
    "## milvus lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37c1f37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URI = \"./milvus_example.db\"\n",
    "\n",
    "# vector_store = Milvus(\n",
    "#     embedding_function=embeddings,\n",
    "#     connection_args={\"uri\": URI},\n",
    "#     index_params={\"index_type\": \"FLAT\", \"metric_type\": \"L2\"},\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d415c6a8",
   "metadata": {},
   "source": [
    "## create the milvus db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cb92f8",
   "metadata": {},
   "source": [
    "### 1) install the docker[install](https://docs.docker.com/desktop/setup/install/windows-install/)\n",
    "### 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "931684bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tqdm import tqdm\n",
    "from pymilvus import utility, Collection,connections,CollectionSchema, FieldSchema, DataType,db\n",
    "from langchain.vectorstores import Milvus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b2514b38",
   "metadata": {},
   "outputs": [
    {
     "ename": "MilvusException",
     "evalue": "<MilvusException: (code=2, message=Fail connecting to server on 127.0.0.1:19530, illegal connection params or server unavailable)>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFutureTimeoutError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\kumar\\.conda\\envs\\myenv\\Lib\\site-packages\\pymilvus\\client\\grpc_handler.py:151\u001b[0m, in \u001b[0;36mGrpcHandler._wait_for_channel_ready\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 151\u001b[0m     \u001b[43mgrpc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchannel_ready_future\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_channel\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setup_identifier_interceptor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_user, timeout\u001b[38;5;241m=\u001b[39mtimeout)\n",
      "File \u001b[1;32mc:\\Users\\kumar\\.conda\\envs\\myenv\\Lib\\site-packages\\grpc\\_utilities.py:162\u001b[0m, in \u001b[0;36m_ChannelReadyFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mresult\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 162\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kumar\\.conda\\envs\\myenv\\Lib\\site-packages\\grpc\\_utilities.py:106\u001b[0m, in \u001b[0;36m_ChannelReadyFuture._block\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 106\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mFutureTimeoutError()\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mFutureTimeoutError\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mMilvusException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpymilvus\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Collection, MilvusException, connections, db, utility\n\u001b[1;32m----> 3\u001b[0m conn \u001b[38;5;241m=\u001b[39m \u001b[43mconnections\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m127.0.0.1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m19530\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Check if the database exists\u001b[39;00m\n\u001b[0;32m      6\u001b[0m db_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmilvus_demo\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\kumar\\.conda\\envs\\myenv\\Lib\\site-packages\\pymilvus\\orm\\connections.py:461\u001b[0m, in \u001b[0;36mConnections.connect\u001b[1;34m(self, alias, user, password, db_name, token, _async, **kwargs)\u001b[0m\n\u001b[0;32m    458\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m parsed_uri\u001b[38;5;241m.\u001b[39mscheme \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    459\u001b[0m             kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msecure\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 461\u001b[0m     \u001b[43mconnect_milvus\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpassword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdb_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdb_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    462\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    464\u001b[0m \u001b[38;5;66;03m# 2nd Priority, connection configs from env\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kumar\\.conda\\envs\\myenv\\Lib\\site-packages\\pymilvus\\orm\\connections.py:411\u001b[0m, in \u001b[0;36mConnections.connect.<locals>.connect_milvus\u001b[1;34m(**kwargs)\u001b[0m\n\u001b[0;32m    408\u001b[0m timeout \u001b[38;5;241m=\u001b[39m t \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, (\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m)) \u001b[38;5;28;01melse\u001b[39;00m Config\u001b[38;5;241m.\u001b[39mMILVUS_CONN_TIMEOUT\n\u001b[0;32m    410\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _async:\n\u001b[1;32m--> 411\u001b[0m     \u001b[43mgh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_for_channel_ready\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    413\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeep_alive\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    414\u001b[0m     gh\u001b[38;5;241m.\u001b[39mregister_state_change_callback(\n\u001b[0;32m    415\u001b[0m         ReconnectHandler(\u001b[38;5;28mself\u001b[39m, alias, kwargs_copy)\u001b[38;5;241m.\u001b[39mreconnect_on_idle\n\u001b[0;32m    416\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\kumar\\.conda\\envs\\myenv\\Lib\\site-packages\\pymilvus\\client\\grpc_handler.py:155\u001b[0m, in \u001b[0;36mGrpcHandler._wait_for_channel_ready\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mFutureTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m--> 155\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MilvusException(\n\u001b[0;32m    156\u001b[0m         code\u001b[38;5;241m=\u001b[39mStatus\u001b[38;5;241m.\u001b[39mCONNECT_FAILED,\n\u001b[0;32m    157\u001b[0m         message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFail connecting to server on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_address\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, illegal connection params or server unavailable\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    158\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    160\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[1;31mMilvusException\u001b[0m: <MilvusException: (code=2, message=Fail connecting to server on 127.0.0.1:19530, illegal connection params or server unavailable)>"
     ]
    }
   ],
   "source": [
    "from pymilvus import Collection, MilvusException, connections, db, utility\n",
    "\n",
    "conn = connections.connect(host=\"127.0.0.1\", port=19530)\n",
    "\n",
    "# Check if the database exists\n",
    "db_name = \"milvus_demo\"\n",
    "try:\n",
    "    existing_databases = db.list_database()\n",
    "    if db_name in existing_databases:\n",
    "        print(f\"Database '{db_name}' already exists.\")\n",
    "\n",
    "        # Use the database context\n",
    "        db.using_database(db_name)\n",
    "\n",
    "        # Drop all collections in the database\n",
    "        collections = utility.list_collections()\n",
    "        for collection_name in collections:\n",
    "            collection = Collection(name=collection_name)\n",
    "            collection.drop()\n",
    "            print(f\"Collection '{collection_name}' has been dropped.\")\n",
    "\n",
    "        db.drop_database(db_name)\n",
    "        print(f\"Database '{db_name}' has been deleted.\")\n",
    "    else:\n",
    "        print(f\"Database '{db_name}' does not exist.\")\n",
    "        database = db.create_database(db_name)\n",
    "        print(f\"Database '{db_name}' created successfully.\")\n",
    "except MilvusException as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463cc9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import MilvusClient\n",
    "import numpy as np\n",
    "\n",
    "client = MilvusClient(\"./milvus_demo.db\")\n",
    "client.create_collection(\n",
    "    collection_name=\"demo_collection\",\n",
    "    dimension=384  # The vectors we will use in this demo has 384 dimensions\n",
    ")\n",
    "\n",
    "docs = [\n",
    "    \"Artificial intelligence was founded as an academic discipline in 1956.\",\n",
    "    \"Alan Turing was the first person to conduct substantial research in AI.\",\n",
    "    \"Born in Maida Vale, London, Turing was raised in southern England.\",\n",
    "]\n",
    "\n",
    "vectors = [[ np.random.uniform(-1, 1) for _ in range(384) ] for _ in range(len(docs)) ]\n",
    "data = [ {\"id\": i, \"vector\": vectors[i], \"text\": docs[i], \"subject\": \"history\"} for i in range(len(vectors)) ]\n",
    "res = client.insert(\n",
    "    collection_name=\"demo_collection\",\n",
    "    data=data\n",
    ")\n",
    "\n",
    "res = client.search(\n",
    "    collection_name=\"demo_collection\",\n",
    "    data=[vectors[0]],\n",
    "    filter=\"subject == 'history'\",\n",
    "    limit=2,\n",
    "    output_fields=[\"text\", \"subject\"],\n",
    ")\n",
    "print(res)\n",
    "\n",
    "res = client.query(\n",
    "    collection_name=\"demo_collection\",\n",
    "    filter=\"subject == 'history'\",\n",
    "    output_fields=[\"text\", \"subject\"],\n",
    ")\n",
    "print(res)\n",
    "\n",
    "res = client.delete(\n",
    "    collection_name=\"demo_collection\",\n",
    "    filter=\"subject == 'history'\",\n",
    ")\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60645251",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import MilvusClient\n",
    "\n",
    "\n",
    "MILVUS_URI = \"./huggingface_milvus_test.db\"  # Connection URI\n",
    "COLLECTION_NAME = \"huggingface_test\"  # Collection name\n",
    "DIMENSION = 384  # Embedding dimension depending on model\n",
    "\n",
    "milvus_client = MilvusClient(MILVUS_URI)\n",
    "if milvus_client.has_collection(collection_name=COLLECTION_NAME):\n",
    "    milvus_client.drop_collection(collection_name=COLLECTION_NAME)\n",
    "milvus_client.create_collection(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    dimension=DIMENSION,\n",
    "    auto_id=True,  # Enable auto id\n",
    "    enable_dynamic_field=True,  # Enable dynamic fields\n",
    "    vector_field_name=\"question_embedding\",  # Map vector field name and embedding column in dataset\n",
    "    consistency_level=\"Strong\",  # To enable search with latest data\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f6e1fd",
   "metadata": {},
   "source": [
    "## hugging face enbeddings with neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81706413",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install neo4j==5.26.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e3522bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NEO4J_URI='neo4j+s://b9042d2c.databases.neo4j.io'\n",
    "#NEO4J_URI='neo4j://localhost:7687'\n",
    "NEO4J_USERNAME='neo4j'\n",
    "NEO4J_PASSWORD='XMm9obeSnhW3SUhwyAosA2z-ljNfGPj3ycQoc9Xo8WI'\n",
    "##AURA_INSTANCEID=b9042d2c\n",
    "#AURA_INSTANCENAME=Instance01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca7b840e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_huggingface import HuggingFaceEmbeddings \n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores.neo4j_vector import Neo4jVector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "416f63af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kumar\\.conda\\envs\\myenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "model=\"BAAI/bge-large-en\"\n",
    "embeddings=HuggingFaceEmbeddings(model_name=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ac82231",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e28a356",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: The query used a deprecated procedure. ('db.index.vector.createNodeIndex' has been replaced by 'CREATE VECTOR INDEX')} {position: line: 1, column: 1, offset: 0} for query: 'CALL db.index.vector.createNodeIndex($index_name,$node_label,$embedding_node_property,toInteger($embedding_dimension),$similarity_metric )'\n",
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: CALL subquery without a variable scope clause is now deprecated. Use CALL (row) { ... }} {position: line: 1, column: 21, offset: 20} for query: \"UNWIND $data AS row CALL { WITH row MERGE (c:`techqa` {id: row.id}) WITH c, row CALL db.create.setNodeVectorProperty(c, 'embedding', row.embedding) SET c.`text` = row.text SET c += row.metadata } IN TRANSACTIONS OF 1000 ROWS \"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "FILE_PATH=\"data1\\pages.pdf\"\n",
    "\n",
    "loader = PyPDFLoader(FILE_PATH)\n",
    "docs = loader.load()\n",
    "\n",
    "# Create a text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500,chunk_overlap=200)\n",
    "\n",
    "# Split documents into chunks\n",
    "chunks = text_splitter.split_documents(docs)\n",
    "\n",
    "# Create a Neo4j vector store\n",
    "neo4j_db = Neo4jVector.from_documents(\n",
    "    chunks,\n",
    "    embeddings,\n",
    "    url=NEO4J_URI,\n",
    "    username=NEO4J_USERNAME,\n",
    "    password=NEO4J_PASSWORD,\n",
    "    database='neo4j',\n",
    "    index_name='techqaVector',\n",
    "    node_label='techqa',\n",
    "    text_node_property='text',\n",
    "    embedding_node_property='embedding',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36064a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 2 of 4 Creating PDF Documents \n",
      " \n",
      " \n",
      "Option 1 – Use Adobe PDF Printer Command: \n",
      "In many authoring applications, yo u can use the Print command with \n",
      "the Adobe PDF printer to convert your file to PDF.   Create a PDF using the Print command (Windows) \n",
      "1. Open the file in its authoring a pplication, and choose File > Print. \n",
      "2. Choose Adobe PDF from the printer menu. \n",
      " \n",
      " \n",
      " \n",
      "3. Click the Properties (or Preferen ces) button to customize the Adobe \n",
      "PDF printer setting. (In some app lications, you may need to click \n",
      "Setup in the Print dialog box to open  the list of printers, and then click \n",
      "Properties or Preferences.)  Choose Sm allest File Size as your default \n",
      "setting. \n",
      " \n",
      " \n",
      " \n",
      "4. In the Print dialog box, click OK and Save your file. \n",
      " \n",
      " \n",
      " \n",
      "Create a PDF using the Print command (Mac OS) \n",
      "1. Open the file in its authoring a pplication, and choose File > Print. \n",
      "2. Click on the PDF button in the Print window. 3. Click Save as PDF.\n"
     ]
    }
   ],
   "source": [
    "question=\"how to create a pdf document\"\n",
    "response = neo4j_db.similarity_search(question)\n",
    "\n",
    "print(response[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc24f4f-d23c-4202-992d-91b0623136ae",
   "metadata": {},
   "source": [
    "## How to execute the code from Visual Studio Code\n",
    "* In Visual Studio Code, see the file 004-vector-stores.py\n",
    "* In terminal, make sure you are in the directory of the file and run:\n",
    "    * python 004-vector-stores.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1af248e-6069-44b3-a2cd-a20aa3259874",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
