{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "465c003a-29cf-4cfe-a0c6-d36c04ae2b37",
   "metadata": {},
   "source": [
    "# Vector Stores (aka. Vector Databases)\n",
    "* Store embeddings in a very fast searchable database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5514113-ddca-4ae9-9de6-0b9225b18f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fecd39d0-e72e-4bc2-8a68-2fa4008ea365",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f4a923-b19e-498e-9be5-e47ec4a77d80",
   "metadata": {},
   "source": [
    "#### Install LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fef1e5c-b7e2-4a04-96c5-8f64377b8eba",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1cf94ae-6c39-4475-9c5b-4b74d8d78753",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189e9e17-dfb0-4fd3-85b9-1fba83771941",
   "metadata": {},
   "source": [
    "## Connect with an LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21d23f4-61f5-4227-8a75-7eefde6680ee",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "148df8e0-361d-4ddd-8709-af48fa1648d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1998155-91de-4cbc-bc88-8d77beefb51b",
   "metadata": {},
   "source": [
    "* NOTE: Since right now is the best LLM in the market, we will use OpenAI by default. You will see how to connect with other Open Source LLMs like Llama3 or Mistral in a next lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "209b683b-5edc-4418-9468-34c12f1172d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chatModel = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c811b84c-7054-49dd-b0ac-30f327f648a0",
   "metadata": {},
   "source": [
    "## Reminder: Steps of the RAG process.\n",
    "* When you load a document, you end up with strings. Sometimes the strings will be too large to fit into the context window. In those occassions we will use the RAG technique:\n",
    "    * Split document in small chunks.\n",
    "    * Transform text chunks in numeric chunks (embeddings).\n",
    "    * **Load embeddings to a vector database (aka vector store)**.\n",
    "    * Load question and retrieve the most relevant embeddings to respond it.\n",
    "    * Sent the embeddings to the LLM to format the response properly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c573cdd8-3aff-488b-8649-4050a42e26c4",
   "metadata": {},
   "source": [
    "## Vector databases (aka vector stores): store and search embeddings\n",
    "* See the documentation page [here](https://python.langchain.com/v0.1/docs/modules/data_connection/vectorstores/).\n",
    "* See the list of vector stores [here](https://python.langchain.com/v0.1/docs/integrations/vectorstores/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055b5f44-5c40-4eb7-aa25-f1591b0f1580",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25bbb79b-595f-44e8-b146-65152becb321",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain-chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e0c5a1d-10d3-4813-a5e3-765c2127e174",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "# Load the document, split it into chunks, embed each chunk and load it into the vector store.\n",
    "loaded_document = TextLoader('./data/state_of_the_union.txt').load()\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "\n",
    "chunks_of_text = text_splitter.split_documents(loaded_document)\n",
    "\n",
    "vector_db = Chroma.from_documents(chunks_of_text, OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdbd14f2-a3ca-40db-887b-b86d65d0b22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you’re at it, pass the Disclose Act so Americans can know who is funding our elections. \n",
      "\n",
      "Tonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \n",
      "\n",
      "One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \n",
      "\n",
      "And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.\n"
     ]
    }
   ],
   "source": [
    "question = \"What did the president say about the John Lewis Voting Rights Act?\"\n",
    "\n",
    "response = vector_db.similarity_search(question)\n",
    "\n",
    "print(response[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d43131-adf6-4d17-a256-05432a834b29",
   "metadata": {},
   "source": [
    "The `.similarity_search` method in the previous code is used to find the most relevant text chunks from a pre-processed document that are similar to a given query. Here's how it works step-by-step\n",
    "\n",
    "1. **Document Processing and Embedding:**\n",
    "   - The document `state_of_the_union.txt` is loaded using the `TextLoader` from the LangChain Community package.\n",
    "   - The document is then split into smaller chunks of text using the `CharacterTextSplitter`, where each chunk is 1000 characters long without any overlap between the chunks.\n",
    "   - Each chunk of text is then transformed into an embedding using `OpenAIEmbeddings`. Embeddings are high-dimensional vectors that represent the semantic content of the text.\n",
    "   - These embeddings are stored in an instance of `Chroma`, which serves as a vector database optimized for efficient similarity searches.\n",
    "\n",
    "2. **Using `.similarity_search`:**\n",
    "   - When you invoke `vector_db.similarity_search(question)`, the method converts the query (`\"What did the president say about the John Lewis Voting Rights Act?\"`) into an embedding using the same method that was used for the chunks.\n",
    "   - It then searches the vector database (`vector_db`) to find the chunks whose embeddings are most similar to the embedding of the query. This similarity is typically measured using metrics such as cosine similarity.\n",
    "   - The search results are sorted by relevance, with the most relevant chunks (those that are semantically closest to the query) returned first.\n",
    "\n",
    "3. **Output:**\n",
    "   - The result of the `.similarity_search` is stored in `response`, which contains the relevant chunks and their similarity scores.\n",
    "   - The script prints the content of the most relevant chunk (the first result), which should ideally contain information about what the president said regarding the John Lewis Voting Rights Act.\n",
    "\n",
    "This method is particularly useful in applications like question answering or document retrieval where you need to quickly find the most relevant parts of a large text based on a query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc24f4f-d23c-4202-992d-91b0623136ae",
   "metadata": {},
   "source": [
    "## How to execute the code from Visual Studio Code\n",
    "* In Visual Studio Code, see the file 004-vector-stores.py\n",
    "* In terminal, make sure you are in the directory of the file and run:\n",
    "    * python 004-vector-stores.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1af248e-6069-44b3-a2cd-a20aa3259874",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmapp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
